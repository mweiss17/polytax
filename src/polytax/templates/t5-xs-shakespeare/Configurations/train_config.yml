# Model and Paths
cache_dir: "/tmp/"
seed: 12345

# Multi-host
backend: "GLOO" # use NCCL for GPU

# Dataset and Batch Size
dataset:
  train:
    kwargs:
      name: "tiny_shakespeare"
      max_seq_length: 32
      per_device_batch_size: 4
#  validation:
#    name: "glue_v002_proportional" # "glue_cola_v002"
#    kwargs:
#      max_seq_length: 32
#      batch_size: 4

# Optimizer
optim:
  name: "Adafactor"
  kwargs:
    clip_threshold: 1.0
    decay_rate: -0.8
    weight_decay: 0.0
    relative_step: True
    scale_parameter: True
    warmup_init: True


# Steps
training:
  checkpoint_every: 1

# Steps
wandb:
  log_scalars_every: 1

valid_every: 1
num_train_steps: 200
num_eval_steps: 1

# Logging
use_wandb: True

# Model Config
model_config:
  d_ff: 32
  d_kv: 16
  d_model: 32
  decoder_start_token_id: 0
  dropout_rate: 0.1
  eos_token_id: 1
  feed_forward_proj: "relu"
  gradient_checkpointing: "False"
  initializer_factor: 1.0
  is_encoder_decoder: "True"
  layer_norm_epsilon: 1e-6
  model_type: "t5-xs"
  num_decoder_layers: 1
  num_heads: 2
  num_layers: 1
  pad_token_id: 0
  relative_attention_num_buckets: 4
  transformers_version: "4.10.0.dev0"
  use_cache: false
  vocab_size: 32003
