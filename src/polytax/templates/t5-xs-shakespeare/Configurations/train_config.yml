# Model and Paths
model_type: "t5-small"
cache_dir: "/tmp/"
seed: 12345

# Multi-host
backend: "GLOO" # use NCCL for GPU

# Tokenizer
tokenizer:
  kwargs:
    tokenizer_name: "./tokenizer/"
    use_fast: True

# Dataset and Batch Size
dataset_name: "tiny_shakespeare"
max_seq_length: 16
per_device_train_batch_size: 1
per_device_eval_batch_size: 1

# Optimization
adafactor: "True"
learning_rate: 0.005
weight_decay: 0.001
warmup_steps: 2000
dtype: "float32"
adam_beta1: 0.9
adam_beta2: 0.999

# Steps
log_every: 1
checkpoint_every: 1
eval_every: 1
num_train_steps: 20
num_eval_steps: 3

# Logging
wandb:
  use: False
  log_every: 100

# Model Config
model_config:
  d_ff: 32
  d_kv: 16
  d_model: 32
  decoder_start_token_id: 0
  dropout_rate: 0.1
  eos_token_id: 1
  feed_forward_proj: "relu"
  gradient_checkpointing: "False"
  initializer_factor: 1.0
  is_encoder_decoder: "True"
  layer_norm_epsilon: 1e-06
  model_type: "t5-xs"
  num_decoder_layers: 1
  num_heads: 2
  num_layers: 1
  pad_token_id: 0
  relative_attention_num_buckets: 4
  transformers_version: "4.10.0.dev0"
  use_cache: true
  vocab_size: 32003
