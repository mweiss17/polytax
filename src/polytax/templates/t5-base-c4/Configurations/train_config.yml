# Model and Paths
model_type: "t5-base"
cache_dir: "/tmp/"
seed: 12345

# Tokenizer
tokenizer:
  kwargs:
    tokenizer_name: "./tokenizer/"
    use_fast: True

# Dataset and Batch Size
dataset_name: "c4.en"
max_seq_length: 512 
per_device_train_batch_size: 8
per_device_eval_batch_size: 8

# Optimization
adafactor: "True"
learning_rate: 0.005
weight_decay: 0.001
warmup_steps: 2000
dtype: "float32"
adam_beta1: 0.9
adam_beta2: 0.999
gradient_accumulation_steps: 2

# Steps

training:
  checkpoint_every: 10000

# Steps
wandb:
  log_scalars_every: 1000

eval_every: 50
num_train_steps: 1000000
num_eval_steps: 1

# Logging
use_wandb: True

# Model Config
model_config:
  d_ff: 3072
  d_kv: 64
  d_model: 768
  decoder_start_token_id: 0
  dropout_rate: 0.1
  eos_token_id: 1
  feed_forward_proj: "relu"
  gradient_checkpointing: False
  initializer_factor: 1.0
  is_encoder_decoder: "True"
  layer_norm_epsilon: 1e-06
  model_type: "t5"
  num_decoder_layers: 12
  num_heads: 12
  num_layers: 12
  pad_token_id: 0
  relative_attention_num_buckets: 32
  transformers_version: "4.10.0.dev0"
  use_cache: false
  vocab_size: 32003
